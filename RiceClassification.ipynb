{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RiceClassification.ipynb","provenance":[],"collapsed_sections":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ptDJ72JzVv5n"},"outputs":[],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","source":["\n","## Download Rice Dataset\n","from:  https://drive.google.com/file/d/1eSp5f5ih17blcqjgxJQ1IKx9a7QXTqJT/view?usp=sharing"],"metadata":{"id":"cdFK3EzsDkue"}},{"cell_type":"code","source":["!gdown  https://drive.google.com/u/0/uc?id=1eSp5f5ih17blcqjgxJQ1IKx9a7QXTqJT&export=download\n","!unzip ./rise.zip -d ./rice"],"metadata":{"id":"SamMwkvuCnUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get device for training\n","\n","import torch\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))"],"metadata":{"id":"diEoqVlN_U1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Read Images"],"metadata":{"id":"j3q9A0SEEgxA"}},{"cell_type":"code","source":["import os\n","\n","img_dir = '/content/rice/Rice_Image_Dataset/'\n","img_labels = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']  # all possible classes\n","num_classes = len(img_labels)\n","img_files = [[os.path.join(img_dir, label, x) \n","               for x in os.listdir(os.path.join(img_dir, label))] \n","               for label in img_labels]"],"metadata":{"id":"FkDATb4PEoN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(img_labels), len(img_files))"],"metadata":{"id":"5PcLjJ-VJg5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_file_list = []\n","img_label_list = []\n","for i, class_name in enumerate(img_labels):\n","    img_file_list.extend(img_files[i])\n","    img_label_list.extend([i] * len(img_files[i]))"],"metadata":{"id":"PHVUz1Z0E0WR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","\n","# find image dimensions\n","image_width, image_height = Image.open(img_file_list[0]).size\n","print(\"Image dimensions:\", image_width, \"x\", image_height)"],"metadata":{"id":"dHB5q2t1r-Va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(img_file_list), len(img_label_list))\n","total_img = len(img_label_list)"],"metadata":{"id":"FsqYL9C3NFpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(img_label_list)"],"metadata":{"id":"ubn-v3ShMl4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","# Shuffle both lists at once with same order\n","\n","zipped = list(zip(img_file_list, img_label_list))\n","random.shuffle(zipped)\n","img_file_list, img_label_list = zip(*zipped)\n","\n","print(img_label_list)"],"metadata":{"id":"1mCgZ35hM9IQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train, Test, Validation Split"],"metadata":{"id":"N7b4hPH5L7RF"}},{"cell_type":"code","source":["fractions = [0.6, 0.2, 0.2]  # train, test, val\n","length_to_split = [int(f * total_img) for f in fractions]\n","print(length_to_split)"],"metadata":{"id":"jha3hj_4McK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import islice\n","\n","file_input = iter(img_file_list)\n","file_output = [list(islice(file_input, elem)) for elem in length_to_split]\n","print(len(file_output))\n","\n","label_input = iter(img_label_list)\n","label_output = [list(islice(label_input, elem)) for elem in length_to_split]\n","print(len(label_output))"],"metadata":{"id":"b0KtSPV8Qgmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","X_train = np.array(file_output[0])\n","X_test = np.array(file_output[1])\n","X_val = np.array(file_output[2])\n","\n","y_train = np.array(label_output[0])\n","y_test = np.array(label_output[1])\n","y_val = np.array(label_output[2])"],"metadata":{"id":"uYDX8yIKTkdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train)  # example"],"metadata":{"id":"F273L9jST6sH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(X_train), len(y_train))\n","print(len(X_test), len(y_test))\n","print(len(X_val), len(y_val))"],"metadata":{"id":"FtkwewSsUYV6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating a Custom Dataset for my files"],"metadata":{"id":"0F0wAHGtU0ZF"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from torchvision import datasets, transforms, models\n","from torchvision.transforms import ToTensor, Lambda\n","import matplotlib.pyplot as plt"],"metadata":{"id":"QPUa4oU3WR7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomImageDataset(Dataset):\n","\n","  def __init__(self, img_files, img_labels, transforms):\n","    self.img_files = img_files\n","    self.img_labels = img_labels\n","    self.transforms = transforms\n","\n","  def __len__(self):\n","    return len(self.img_labels)\n","\n","  def __getitem__(self, index):\n","    return self.transforms(self.img_files[index]), self.img_labels[index]"],"metadata":{"id":"S6TXR6DqXuQz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing the data for training with DataLoaders"],"metadata":{"id":"qsDRQhYtVBUG"}},{"cell_type":"code","source":["!pip install -q \"monai-weekly[gdown, nibabel, tqdm, itk]\""],"metadata":{"id":"jkjxnwkw3PGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from monai.transforms import *\n","\n","class SumDimension(Transform):\n","    def __init__(self, dim=1):\n","        self.dim = dim\n","\n","    def __call__(self, inputs):\n","        return inputs.sum(self.dim)"],"metadata":{"id":"eT2fT9WV4dhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define transforms\n","\n","transform = Compose([\n","    LoadImage(image_only=True),\n","    AddChannel(),\n","    ScaleIntensity(),\n","    SumDimension(3),\n","    ToTensor(),\n","])\n","\n","act = Activations(softmax=True)\n","to_onehot = AsDiscrete(to_onehot=True, n_classes=num_classes)"],"metadata":{"id":"v5y1QBU3zfeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data = CustomImageDataset(X_train, y_train, transform)\n","test_data = CustomImageDataset(X_test, y_test, transform)\n","val_data = CustomImageDataset(X_val, y_val, transform)"],"metadata":{"id":"VMFJciUrVItT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","val_dataloader = DataLoader(val_data, batch_size=64)"],"metadata":{"id":"a8STe9xmbEmS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Neural Network"],"metadata":{"id":"K2v1Cf-jbHER"}},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"9PJrH8NIb2w0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the Class\n","\n","class NeuralNetwork(nn.Module):\n","  def __init__(self):\n","    super(NeuralNetwork, self).__init__()\n","    self.flatten = nn.Flatten()\n","    self.linear_relu_stack = nn.Sequential(\n","        nn.Linear(image_width*image_height, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, len(img_labels)),\n","        nn.ReLU()\n","    )\n","  \n","  def forward(self, x):\n","    x = self.flatten(x)\n","    logits = self.linear_relu_stack(x)\n","    return logits"],"metadata":{"id":"DWTzeenWfNHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NeuralNetwork().to(device)\n","print(model)"],"metadata":{"id":"o4qjQ54QiFAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = torch.rand(1, image_width, image_height, device=device)\n","logits = model(X)\n","pred_probabilities = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probabilities.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"],"metadata":{"id":"4YZMntcIib_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizing Model Params"],"metadata":{"id":"pEw6b8Gxtwxi"}},{"cell_type":"code","source":["# Loss Function\n","\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"sEG2csvvtrnD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer\n","\n","learning_rate = 1e-5\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"ffPO16jtuhOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Loop\n","\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","  size = len(dataloader.dataset)\n","\n","  # print(\"dataloader: \", dataloader)\n","  for batch, (X, y) in enumerate(dataloader):\n","    X = X.to(device)\n","    y = y.to(device)\n","    \n","    # compute prediction and loss\n","    pred = model(X)\n","    loss = loss_fn(pred, y)\n","\n","    # backpropagation\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if batch % 100 == 0:\n","      loss, current = loss.item(), batch * len(X)\n","      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>d}]\")"],"metadata":{"id":"0KdfAE2iv6Hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test Loop\n","\n","def test_loop(dataloader, model, loss_fn):\n","  size = len(dataloader.dataset)\n","  test_loss, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for X, y in dataloader:\n","      X = X.to(device)\n","      y = y.to(device)\n","      pred = model(X)\n","      test_loss += loss_fn(pred, y).item()\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","  test_loss /= size\n","  correct /= size\n","  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"PDgBVVLcw3hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","\n","epochs = 10\n","for t in range(epochs):\n","  print(f\"Epoch {t+1}\\n------------------------------------\")\n","  train_loop(train_dataloader, model, loss_fn, optimizer)\n","  test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"id":"gcPs0YO4xvxI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving and Loading Model and Model Weights\n"],"metadata":{"id":"-vr81SnAhgP3"}},{"cell_type":"code","source":["# Saving weights\n","\n","model = models.vgg16(pretrained=True)\n","torch.save(model.state_dict(), 'model_weights.pth')"],"metadata":{"id":"3MMnSXmVyXRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading weights\n","\n","model = models.vgg16()\n","model.load_state_dict(torch.load('model_weights.pth'))\n","model.eval()"],"metadata":{"id":"l6NE8McsimDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving model\n","\n","torch.save(model, 'model.pth')"],"metadata":{"id":"Bbf-kN17kG27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loadind model\n","\n","model = torch.load('model.pth')\n","model.eval()"],"metadata":{"id":"dLXoSTvqkoJM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predict Example"],"metadata":{"id":"ffw4AnFEml_n"}},{"cell_type":"code","source":["FOLDERNAME = \"computerVisionAssignments/assignment3\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd drive/My\\ Drive/$FOLDERNAME/"],"metadata":{"id":"p2bjg_msm_qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","\n","def predict(img_path, trans):\n","  image = Image.open(Path(img_path))\n","  input = trans(image)\n","  output = model(input)\n","  prediction = int(torch.max(output.data, 1)[1].numpy())"],"metadata":{"id":"Kg1JYP-Ionpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve item\n","index = 100\n","path = img_files[3][index]  # an image from class number 3 (Jasmin)\n","print(path)\n","\n","# Generate prediction\n","image = transform(path)  # of type tensor\n","new_shape = tuple([1]) + image.shape  # shape: [N, C, W, H]\n","image = torch.reshape(image, new_shape)\n","image = image.permute(0, 3, 1, 2)\n","print(image.shape)\n","prediction = model(image)\n","\n","# Predicted class value using argmax\n","predicted_class = np.argmax(prediction)\n","print(predicted_class)"],"metadata":{"id":"0AXixlMJmljw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BnhmjGjJpATm"},"execution_count":null,"outputs":[]}]}